---
title: "MSCI446 - Assignment 1 - Question 4"
author: "H. Gomma, K. Morris, M. Harper"
date: "05/02/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## 4.0 BGD

```{r}
BGD <- function(x, y, theta0, alpha = 0.01, epsilon = 1e-8, max_iter=25000){
  
  # Inputs
  # x      : The input variables (M columns)
  # y      : Output variables    (1 column)
  # theta0 : Initial weight vector (M+1 columns)
  
  x     <- as.matrix(x)
  y     <- as.matrix(y) 
  N     <- nrow(x)
  i     <- 0
  theta <- theta0
  x     <- cbind(1, x) # Adding 1 as first column for intercept
  imprv <- 1e10
  cost  <- (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y)
  delta <- 1
  while(imprv > epsilon & i < max_iter){cost
    i <- i + 1
    grad <- 0
    for(j in 1:length(y)){
      grad_chng <- x[j, ] * c(y[j]-x[j, ] %*% theta)
      grad <- grad + grad_chng 
    }
    theta <- theta + (alpha/N) * grad
    cost  <- append(cost, (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y))
    imprv <- abs(cost[i+1] - cost[i])
    if((cost[i+1] - cost[i]) > 0) stop("Cost is increasing. Try reducing alpha.")
  }
  print(paste0("Stopped in ", i, " iterations"))
  
  cost <- cost[-1]
  return(list(theta,cost))
}

```

```{r}
library('ISLR')
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
res <- BGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 10)
```

```{r}
library('ggplot2')
theta <- res[[1]]
lossBGD  <- res[[2]]
ggplot() + 
  geom_point(aes(x=1:length(lossBGD), y=lossBGD)) +
  labs(x='iteration')
```

\newpage
## SGD

```{r}
SGD <- function(x, y, theta0, alpha = 0.01, epsilon = 1e-8, max_iter=25000){
  
  # Inputs
  # x      : The input variables (M columns)
  # y      : Output variables    (1 column)
  # theta0 : Initial weight vector (M+1 columns)
  
  x     <- as.matrix(x)
  y     <- as.matrix(y) 
  N     <- nrow(x)
  i     <- 0
  theta <- theta0
  x     <- cbind(1, x) # Adding 1 as first column for intercept
  imprv <- 1e10
  cost  <- (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y)
  delta <- 1
  
  while(imprv > epsilon & i < max_iter){cost
    i <- i + 1
    grad <- 0
    for(j in 1:length(y)){
      grad_chng <- x[j, ] * c(y[j]-x[j, ] %*% theta)
     #grad <- grad + grad_chng  <---- I commented this out
      theta <- theta + (alpha) * grad_chng #brought this in the 
                                               #for loop and changed 
                                               #grad to grad_chng
    }
    #theta <- theta + (alpha / N) * grad
    cost  <- append(cost, (1/(2*N)) * t(x %*% theta - y) %*% (x %*% theta - y))
    imprv <- abs(cost[i+1] - cost[i])
    if((cost[i+1] - cost[i]) > 0) stop("Cost is increasing. Try reducing alpha.")
  }
  
  print(paste0("Stopped in ", i, " iterations"))
  
  cost <- cost[-1]
  return(list(theta,cost))
  
}
```

```{r}
x <- log(Auto$horsepower)
y <- log(Auto$mpg)
res <- SGD(x, y, c(1, -1), alpha = 0.005, epsilon = 1e-5, max_iter = 10)
```

```{r}
theta <- res[[1]]
lossSGD  <- res[[2]]
ggplot() + 
  geom_point(aes(x=1:length(lossSGD), y=lossSGD)) +
  labs(x='iteration')
```
Altering the provided code to SGD involved first commenting out the second line of code in the for loop,
_grad <- grad + grad_chng_. This code sums the changes in gradients from all data points which is than applied to update
our theta values. With SGD, we want to update the theta values with just the grad_chng from that instance. Therefore the theta updating calculation was brought into the for loop and had the variable _grad_ changed to _grad_chng_. Futhermore, we removed the divide by 'N' in the updating as theta, as we are no longer taking the average of the weighted error with respect to all data points (weighted by the predictor variable $X_0$, or $X_1$ for example). Instead we are just updating theta based on the error with respect to a particular data point.


\newpage
## 4.2 Compare BGD and SGD

```{r}
minloss_BGD <- lossBGD[10]
minloss_SGD <- lossSGD[10]

cat('Min loss for BGD: ', minloss_BGD, '\n')
cat('Min loss for SGD: ', minloss_SGD, '\n')
```

It appears as though after 10 iterations of both BDG and SGD, the BGD algorithm actually yielded 
the minimum loss when compared to the SGD algorithm.

```{r}
library('gridExtra')

gBGD <- ggplot() + 
          geom_point(aes(x=1:length(lossBGD), y=lossBGD)) +
          labs(x='iteration') +
          ggtitle('BGD Losses')

gSGD <- ggplot() + 
          geom_point(aes(x=1:length(lossSGD), y=lossSGD)) +
          labs(x='iteration') +
          ggtitle('SGD Losses')

gcombined <- ggplot()+
                geom_point(aes(x=1:length(lossBGD), y=lossBGD), colour='red') +
                geom_point(aes(x=1:length(lossBGD), y=lossSGD), colour='blue') +
                labs(x='iteration', y='loss') +
                ggtitle('BGD(red) vs. SGD(blue) Losses')
                

grid.arrange(gcombined, ncol=1)

```
When observing the two plot above, it is evident that the Stochastic Gradient Descent Algorithm yielded minimum losses. Over 10 iterations, the minimum loss for BGD was approximately 2.23, and was approximately 0.19 for SGD. The vast improvement (reduction) in loss when performing SGD is due to only examining and updating one feature weight at a time which reduces the probability of discrepancies between the output Y and the hypothesis function being calculated with the current weights.


